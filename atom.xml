<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://posierd.github.io</id>
    <title>posierd_Gridea</title>
    <updated>2020-03-07T05:53:14.932Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://posierd.github.io"/>
    <link rel="self" href="https://posierd.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://posierd.github.io/images/avatar.png</logo>
    <icon>https://posierd.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, posierd_Gridea</rights>
    <entry>
        <title type="html"><![CDATA[selenium 模拟浏览器 爬取网易云音乐评论]]></title>
        <id>https://posierd.github.io/post/selenium-mo-ni-liu-lan-qi-pa-qu-wang-yi-yun-yin-le-ping-lun/</id>
        <link href="https://posierd.github.io/post/selenium-mo-ni-liu-lan-qi-pa-qu-wang-yi-yun-yin-le-ping-lun/">
        </link>
        <updated>2020-03-07T05:22:58.000Z</updated>
        <content type="html"><![CDATA[<p>本来相用破解 js来做 但网上找了很多案例 ，还是搞不懂，最后还是用这种达到了我需要的效果</p>
<p>😃😃😃</p>
<pre><code class="language-python">from selenium import webdriver
import time

class Yunspider(object):
    # 初始化的东西 可以在后续的函数中使用
    '''初始化方法'''
    def __init__(self,url):
        '''初始化网址'''
        self.url = url
        # 打开浏览器
        self.driver = webdriver.Chrome()
    # 打开目标网址 提取数据 翻页
    def getContent(self):
        # 打开目标网址
        self.driver.get(self.url)
        # js语句 ，控制下滑的那个进度条
        js = 'window.scrollBy(0,8000)'
        #  先进入 IFrame
        self.driver.switch_to.frame(0) #  0 表示第一个框
        #  执行这条 js 语句
        self.driver.execute_script(js)

        # 提取数据   翻页

        for page in range(0,6):
            selectors = self.driver.find_elements_by_xpath('//div[@class=&quot;cmmts j-flag&quot;]/div')
            for selector in selectors:
                text = selector.find_element_by_xpath('.//div[@class=&quot;cnt f-brk&quot;]').text
                print(text)
                self.save(text)
                #Yunspider.save(text)  也可以调用


            # find_element_by_partial_link_text  模糊匹配
            newpage = self.driver.find_element_by_partial_link_text('下一页')
            newpage.click()
            time.sleep(2)
    @staticmethod    #  静态写法
    def save(item):
        with open(r'C:\Users\DELL\Desktop\python_wd\文本信息\网易云评论.txt','a',encoding='utf-8')as f:
            f.write(&quot;\n&quot;)
            f.write(item)
            f.write(&quot;\n &quot;)




if __name__ == '__main__':
    url = 'https://music.163.com/#/song?id=491274636'
    Yunspider = Yunspider(url)
    Yunspider.getContent()

</code></pre>
<p>运行代码数据部分结果：<br>
<img src="https://posierd.github.io/post-images/1583558646684.png" alt="" loading="lazy"></p>
<p>————————END</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[selenium  模拟浏览器点击翻页]]></title>
        <id>https://posierd.github.io/post/selenium-mo-ni-liu-lan-qi-dian-ji-fan-ye/</id>
        <link href="https://posierd.github.io/post/selenium-mo-ni-liu-lan-qi-dian-ji-fan-ye/">
        </link>
        <updated>2020-03-07T05:18:22.000Z</updated>
        <content type="html"><![CDATA[<pre><code class="language-python">from selenium import webdriver
import time
driver = webdriver.Chrome()
driver.get('https://www.baidu.com/s?ie=UTF-8&amp;wd=python')
for page in range(1,3):
    time.sleep(10)
    nwepage = driver.find_element_by_partial_link_text('下一页')
    nwepage.click()
    time.sleep(10)
#driver.quit()
</code></pre>
<p>运行代码没有报错：<br>
那段js不会，观察内容变化也可以知道！ ，他翻页了<br>
———————— END😊😃😃</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[selenium 模拟鼠标点击收搜结果]]></title>
        <id>https://posierd.github.io/post/selenium-mo-ni-shu-biao-dian-ji-shou-sou-jie-guo/</id>
        <link href="https://posierd.github.io/post/selenium-mo-ni-shu-biao-dian-ji-shou-sou-jie-guo/">
        </link>
        <updated>2020-03-07T05:12:47.000Z</updated>
        <content type="html"><![CDATA[<pre><code class="language-python">from selenium import webdriver
import time
dr = webdriver.Chrome()  #  初始化 Chrome 浏览器实例
dr.get('http://www.baidu.com')  # 打开百度首页
dr.find_element_by_id('kw').send_keys('美女')  # 定位输入框并在输入框中输入关键字
dr.find_element_by_id('su').click()
# 定位 点击 搜索  按钮  ，并模拟点击
# 模糊查找
na = dr.find_element_by_partial_link_text('_海量精选高清图片_百度图片')
na.click()  # 模拟点击
time.sleep(10)  # 理解为暂停 10 秒
dr.quit()  # 关闭浏览器
# dr.refresh() 网页刷新

</code></pre>
<p>运行结果：</p>
<p>不会上动图，但运行代码是没有错误的！！！，是我期望的。。</p>
<p>————————————END</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[爬取荣耀英雄图片]]></title>
        <id>https://posierd.github.io/post/pa-qu-rong-yao-ying-xiong-tu-pian/</id>
        <link href="https://posierd.github.io/post/pa-qu-rong-yao-ying-xiong-tu-pian/">
        </link>
        <updated>2020-03-07T05:05:16.000Z</updated>
        <content type="html"><![CDATA[<p>目标地址就不放了</p>
<p>😄😄😄</p>
<pre><code class="language-python">import requests
import json
import time
start_time = time.time()
link = 'https://pvp.qq.com/web201605/js/herolist.json'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'
}
r = requests.get(link,headers=headers)
print(&quot;页面状态响应码: &quot;,r.status_code)
html = r.text
list = json.loads(html)
for img in list:
    bianhao = img['ename']
    name = img['cname']
    try:
        pifu = img['skin_name'].split('|') # 切割 用来计算有多少皮肤
    except Exception as b:
        print(b)
    for mess in range(1,len(pifu) + 1):
       img_url = ' http://game.gtimg.cn/images/yxzj/img201606/skin/hero-info/' + str(bianhao) + '/'+str(bianhao)+'-bigskin-{}.jpg'.format(mess)
       img_content = requests.get(img_url,headers=headers).content
       with open(r'C:\Users\DELL\Desktop\python_wd\mig\荣耀图片\\'+name + '-' + pifu[mess - 1]+ '.jpg','wb')as f:
           print(&quot;正在下载图片：&quot;, name + '-' + pifu[mess - 1])
           f.write(img_content)
end_time = time.time()
all_time = start_time + end_time
print(&quot;程序耗时（单位：秒）：&quot;,all_time)
print(&quot;下载完成。。。&quot;)

</code></pre>
<p>运行代码数据部分结果：<br>
<img src="https://posierd.github.io/post-images/1583557696227.png" alt="" loading="lazy"></p>
<p>——————END</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[房源信息—郑州 存入 .xlsx文件中]]></title>
        <id>https://posierd.github.io/post/fang-yuan-xin-xi-zheng-zhou-cun-ru-xlsx-wen-jian-zhong/</id>
        <link href="https://posierd.github.io/post/fang-yuan-xin-xi-zheng-zhou-cun-ru-xlsx-wen-jian-zhong/">
        </link>
        <updated>2020-03-07T04:51:44.000Z</updated>
        <content type="html"><![CDATA[<p><a href="https://zz.lianjia.com/ershoufang/">目标地址</a></p>
<p>😄😄😃</p>
<pre><code class="language-python">import requests
from bs4 import BeautifulSoup
import openpyxl

# https://zz.lianjia.com/ershoufang/
# https://zz.lianjia.com/ershoufang/pg2/
# https://zz.lianjia.com/ershoufang/pg3/

list = []
for page in range(1,3):
    print(&quot;正在爬取{}页数据&quot;.format(page))
    link = 'https://zz.lianjia.com/ershoufang/pg+str(page)/'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.116 Safari/537.36'
    }
    r = requests.get(link,headers=headers)
    print(&quot;页面状态响应码: &quot;,r.status_code)


    soup = BeautifulSoup(r.text,'html.parser')
    #一个房子的总数据 最小
    host_list = soup.find_all('div',class_='info clear')
    for host in host_list:
        name = host.find('a',class_='').get_text()
        jinjia = host.find('div',class_='unitPrice').get_text()
        zongjia = host.find('div',class_='totalPrice').get_text()
        ip = host.find('div',class_='positionInfo').get_text()
        guige = host.find('div',class_='houseInfo').get_text()
        guige = guige.replace('|','')
        miaoshu = host.find('div',class_='tag').get_text()
        #print(miaoshu)
        list.append([name,jinjia,zongjia,ip,guige,miaoshu])




# 保存数据
file = openpyxl.Workbook()   #  新建一个工作薄
sheet = file.active     #  在建立工作薄后建立工作表
sheet.title = '房源信息'   #  命名
# 写入内容 格式
sheet['A1'] = '名字'
sheet['B1'] = '均价'
sheet['C1'] = '总价'
sheet['D1'] = '地址'
sheet['E1'] = '规格'
sheet['F1'] = '描述'

for i in list:
    print(i)
    sheet.append(i)
file.save(r&quot;C:\Users\DELL\Desktop\python_wd\文本信息\房源信息-郑州.xlsx&quot;)  #  保存自定义目录



</code></pre>
<p>运行代码数据部分结果：<br>
<img src="https://posierd.github.io/post-images/1583557146044.png" alt="" loading="lazy"></p>
<p>————————END</p>
]]></content>
    </entry>
</feed>